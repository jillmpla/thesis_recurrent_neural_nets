{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorboard via command-line:\n",
    "#tensorboard --logdir=\\where your Logs directory is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "#What is the maximum flare class produced by an AR in the next 24hrs after a 24hr time sequence?\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "#feature data from - hmi.sharp_720s:::\n",
    "#http://jsoc.stanford.edu/doc/data/hmi/sharp/sharp.htm\n",
    "\n",
    "#label data from - GOES flare events:::\n",
    "#ftp://ftp.swpc.noaa.gov/pub/warehouse/\n",
    "\n",
    "#and from - GOES XRS Report:::\n",
    "#https://www.ngdc.noaa.gov/stp/space-weather/solar-data/solar-features/solar-flares/x-rays/goes/xrs/\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "#binary class - F for flare and N for no flare\n",
    "#multi-class - #6 flare classes: N is no flare, A is smallest, X is largest\n",
    "               #key = {'N', 'A', 'B', 'C', 'M', 'X'}\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"tensorflow version:\", tf.__version__) #2.3.1 used\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from astropy.time import Time\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.layers import Dropout, Dense, Embedding, Flatten, SimpleRNN, LSTM, GRU\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "import drms #https://pypi.org/project/drms/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Flare_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################creates/loads csv with 2012's feature data##########\n",
    "current_di = os.getcwd()\n",
    "csv_for_2012 = \"\\\\create_2012_features.csv\"\n",
    "where_2012 = current_di + csv_for_2012\n",
    "\n",
    "if not os.path.exists(where_2012):\n",
    "    import to_create_2012\n",
    "    to_create_2012.get_2012_Features()\n",
    "    df_2012 = pd.read_csv('create_2012_features.csv', index_col=0)\n",
    "    df_2012_done = Flare_Data.convert_time_2012(df_2012)\n",
    "    Flare_Data.save_some_features(df_2012_done) \n",
    "\n",
    "if os.path.exists(where_2012):\n",
    "    df_2012 = pd.read_csv('create_2012_features.csv', index_col=0)\n",
    "    df_2012_done = Flare_Data.convert_time_2012(df_2012)\n",
    "    Flare_Data.save_some_features(df_2012_done)\n",
    "####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################creates/loads csv with 2013's feature data##########\n",
    "current_di = os.getcwd()\n",
    "csv_for_2013 = \"\\\\create_2013_features.csv\"\n",
    "where_2013 = current_di + csv_for_2013\n",
    "\n",
    "if not os.path.exists(where_2013):\n",
    "    import to_create_2013\n",
    "    to_create_2013.get_2013_Features()\n",
    "    df_2013 = pd.read_csv('create_2013_features.csv', index_col=0)\n",
    "    df_2013_done = Flare_Data.convert_time_2013(df_2013)\n",
    "    Flare_Data.save_some_features(df_2013_done) \n",
    "\n",
    "if os.path.exists(where_2013):\n",
    "    df_2013 = pd.read_csv('create_2013_features.csv', index_col=0)\n",
    "    df_2013_done = Flare_Data.convert_time_2013(df_2013)\n",
    "    Flare_Data.save_some_features(df_2013_done)\n",
    "####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################creates/loads csv with 2014's feature data##########\n",
    "current_di = os.getcwd()\n",
    "csv_for_2014 = \"\\\\create_2014_features.csv\"\n",
    "where_2014 = current_di + csv_for_2014\n",
    "\n",
    "if not os.path.exists(where_2014):\n",
    "    import to_create_2014\n",
    "    to_create_2014.get_2014_Features()\n",
    "    df_2014 = pd.read_csv('create_2014_features.csv', index_col=0)\n",
    "    df_2014_done = Flare_Data.convert_time_2014(df_2014)\n",
    "    Flare_Data.save_some_features(df_2014_done) \n",
    "\n",
    "if os.path.exists(where_2014):\n",
    "    df_2014 = pd.read_csv('create_2014_features.csv', index_col=0)\n",
    "    df_2014_done = Flare_Data.convert_time_2014(df_2014)\n",
    "    Flare_Data.save_some_features(df_2014_done)\n",
    "####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################creates/loads csv with 2015's feature data##########\n",
    "current_di = os.getcwd()\n",
    "csv_for_2015 = \"\\\\create_2015_features.csv\"\n",
    "where_2015 = current_di + csv_for_2015\n",
    "\n",
    "if not os.path.exists(where_2015):\n",
    "    import to_create_2015\n",
    "    to_create_2015.get_2015_Features()\n",
    "    df_2015 = pd.read_csv('create_2015_features.csv', index_col=0)\n",
    "    df_2015_done = Flare_Data.convert_time_2015(df_2015)\n",
    "    Flare_Data.save_some_features(df_2015_done) \n",
    "\n",
    "if os.path.exists(where_2015):\n",
    "    df_2015 = pd.read_csv('create_2015_features.csv', index_col=0)\n",
    "    df_2015_done = Flare_Data.convert_time_2015(df_2015)\n",
    "    Flare_Data.save_some_features(df_2015_done)\n",
    "####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removes old tensorboard Logs and recreates or creates Logs folder\n",
    "current_dir = os.getcwd()\n",
    "l_folder = \"\\\\Logs\"\n",
    "where_logs = current_dir + l_folder\n",
    "\n",
    "if not os.path.exists(where_logs):\n",
    "    os.mkdir(where_logs)\n",
    "    \n",
    "if os.path.exists(where_logs):\n",
    "    shutil.rmtree(where_logs)\n",
    "    os.mkdir(where_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binary = True for binary classification, binary = False for multi-class classification\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, tim_steps, n_feats, count_of_classes = Flare_Data.getAllData(binary = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_val.shape, X_test.shape, y_train.shape, y_val.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 50 #do 100 later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~[LSTM2 Model]~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~[Multi-Class]~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~[Multiple Layers]~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(tim_steps, n_feats)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(count_of_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_call = keras.callbacks.TensorBoard(log_dir=where_logs, histogram_freq=1) #update_freq=\"epoch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs, \n",
    "                    validation_data=(X_val, y_val), \n",
    "                    verbose=2, \n",
    "                    callbacks=[tensorboard_call])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate model on test set\n",
    "results = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=2)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate predictions\n",
    "predictions = model.predict(X_test)\n",
    "print(\"predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove encoding\n",
    "real_predictions = np.argmax(predictions, axis=1)\n",
    "print(real_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction counts\n",
    "unique, counts = np.unique(real_predictions, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "matrix = confusion_matrix(y_test.argmax(axis=1), predictions.argmax(axis=1))\n",
    "print(matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
